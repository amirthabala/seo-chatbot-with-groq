# -*- coding: utf-8 -*-
"""Group10_SEO_main_V8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rhbx3vvnaqa2udxPFa2VN4mSUH9uAd3d
"""

import re
import json
import requests
import gradio as gr
from bs4 import BeautifulSoup
from urllib import robotparser
from serpapi import GoogleSearch
from dotenv import dotenv_values
from langchain.tools import Tool
from langchain_groq import ChatGroq
from urllib.parse import urlparse, urljoin
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_community.document_loaders import PyPDFLoader
from langchain.agents import AgentExecutor, create_react_agent
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper

env_vars = dotenv_values(".env")

# API Keys and Creds
serp_api_key = env_vars.get('SERP_API_KEY')
dfs_login = env_vars.get("DATAFORSEO_LOGIN")
dfs_password = env_vars.get("DATAFORSEO_PASSWORD")
groq_api_key = env_vars.get('GROQ_API_KEY')

print("loaded api keys")

# Initializing Groq LLM
GROQ_LLM = ChatGroq(model="llama-3.3-70b-versatile", api_key=groq_api_key)

# Function to interact with the LLM for getting website url
def get_website_url_using_llm_json(message):
    """Use the LLM to get website url if incase some keywords are given as input from the user rather than complete url"""
    instructions = (
        "Extract only the URL from this message. Isolate only the URL related words from the rest of the sentence."
        "The URL may be http or https. Return only the URL"
        "Remove unnecessary words like analyze 'find', 'SEO information', 'with respect to', etc."
        "The URL may not be complete. Complete the URL."
        "Check multiple URL options with different domain endings such as .com, .in, .eu, .org etc if you are unsure of the complete URL"
        "Do not return multiple URL options if the first URL is valid"
        "Please return the following as a JSON object with the key 'urls':"

        "1- list of possible urls"
        "please return only the JSON object. if you don't find any values, return only the key with no values"
    )

    messages = [
        {"role": "system", "content": "You are a URL finder and completer."},
        {"role": "user", "content": instructions},
        {"role": "user", "content": message}
    ]

    try:
        # Call LLM for Getting URL
        response = GROQ_LLM.invoke(
            input=messages,
            temperature=.0001,
            max_tokens=1024,
            top_p=1,
        )
        response_json= json.loads(response.content)
        return response_json  # Return the response from LLM

    except Exception as e:
        return f"Error: {str(e)}"

# Function to extract only urls from the given text
def extract_website_url(text):
    return re.findall(r'(https?://\S+)', text)

# Function to check whether the url is valid or doesn't exist or returns 404
def is_url_not_found(url):
    try:
        response = requests.get(url, allow_redirects=True)
        print(response.status_code)
        if response.status_code in [404]:
            return True

    except requests.RequestException as e:
        print("Can't Scrape the website")
        return None

    return False

# Function to check active websites from the list of URLs
def get_valid_urls(urls):
    valid_urls = []

    condition = lambda value: value != None and not value

    if len(urls) == 1:
        validate = is_url_not_found(urls[0])
        return urls if condition(validate) else []

    for url in urls:
        validate = is_url_not_found(url)
        print(validate)
        if condition(validate):
            valid_urls.append(url)

    return valid_urls

# Function to finalize the website link to analyze seo data
def finalize_website_link(urls_list):
    """
    Returns ->
        case 1: first element in list if urls_list has length one
        case 2: user selected url when prompted if urls_list has length more than one
    """

    urls_list_len = len(urls_list)
    if urls_list_len == 1:
        return urls_list[0]
    else:
        return urls_list

# Function to check whether the website is secured
def is_secured_website(url):
    print("\nchecking for ssl")
    return url.startswith("https://")

# Function to check whether the website is indexed by Google or not using DataForSEO api
def is_indexed_website(url):
    print("\nchecking for indexing")
    # Initializing DataForSEO API
    DFS = DataForSeoAPIWrapper(api_login=dfs_login, api_password=dfs_password, json_result_types=["organic"], json_result_fields=["type"], top_count=1)

    try:
        DFS.results(f"site:{url}")
        return True
    except:
        return False

# Function to trigger Google's Pagespeed API
def call_pagespeed_api(url, device):
    req_url = f'https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&strategy={device}&category=PERFORMANCE'
    res = requests.get(req_url)
    return res.json()

# Function to get the load time value from lighthouse metrics
def get_load_time(data, param):
    time = data.get(param, {}).get("numericValue", 0) / 1000  # convert ms to seconds
    return round(time, 2)

# Function to get the page speed related metrics for desktop
def get_page_speed_metrics(url):
    print("\nchecking for page speed metrics")
    page_data = call_pagespeed_api(url, "DESKTOP")
    metrics = {
        "performance_score": round(page_data.get("lighthouseResult", {}).get("categories", {}).get("performance", {}).get("score", 0) * 100),
        "page_load_category": page_data.get("loadingExperience", {}).get("overall_category", "")
    }

    for metric in ["first-contentful-paint", "largest-contentful-paint", "speed-index", "total-blocking-time"]:
        metrics[metric] = get_load_time(page_data.get("lighthouseResult", {}).get("audits", {}), metric)

    return metrics

# Function to check mobile friendliness of the website
def get_mobile_friendliness_category(url):
    print("\nchecking for mobile friendliness")
    page_data = call_pagespeed_api(url, "MOBILE")
    performance_score = page_data.get("lighthouseResult", {}).get("categories", {}).get("performance", {}).get("score", 0) * 100

    if performance_score >= 90:
        return "Good"
    elif performance_score >= 50 and performance_score < 90:
        return "Needs Improvement"
    else:
        return "Poor"

# Function to get Baseurl of the website
def get_baseurl(url):
    print(url)
    extract_url = extract_website_url(url)
    print(extract_url)
    if len(extract_url):
        parsed_url = urlparse(extract_url[0])
        print(parsed_url)
        scheme = parsed_url.scheme if parsed_url.scheme else 'https'
        netloc = parsed_url.netloc if parsed_url.netloc else url
        return scheme + "://" + netloc
    else:
        return None

# Function to extract sitemap files from robots file
def get_sitemap_urls(url):
    baseurl = get_baseurl(url)
    if baseurl is None:
        return []
    rp = robotparser.RobotFileParser()
    rp.set_url(f"{baseurl}/robots.txt")
    rp.read()
    sitemap_urls = rp.site_maps()
    return sitemap_urls if sitemap_urls else []

# Function to check whether sitemap exists or not
def is_sitemap_exist(url):
    sitemap_url = get_sitemap_urls(url)
    return len(sitemap_url) != 0

# Function to get all anchor links from the website provided
def get_anchor_links(url):
    try:
        # Send a request to fetch the content of the website
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code != 200:
            print((f"Failed to fetch the website: {url}"))
            return None

        # Use BeautifulSoup to parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        links = []
        # Extract all a tags
        a_tags = soup.find_all('a')
        base_url = get_baseurl(url)

        for element in a_tags:
            href = element.get('href')

            # Checking whether href has some value
            if href:
                # Checking for relative paths like /contact, /gallery, ../../help
                if href.startswith('/') or href.startswith('.'):
                    href = urljoin(base_url, href)

                # Checking to avoid duplicates and appending to lists only if it starts with http or www
                # Doing it to ignore href values like #, javascript::, void()
                if not (href in links) and (href.startswith('http') or href.startswith('www')):
                    links.append(href)

        return links
    except requests.RequestException as e:
        return None

# Function to check for broken links in sitemaps
def check_broken_links(url):
    print("\nchecking for broken links", url)
    anchor_links = get_anchor_links(url)
    print(anchor_links)
    broken_urls = []

    if anchor_links:
        anchor_links = anchor_links[:10]
        for i, link in enumerate(anchor_links[:10]): # Limiting to 10 just to speed up process
            print(f"checking link {i + 1} of {len(anchor_links)}")
            if is_url_not_found(url):
                broken_urls.append(url)

        return broken_urls

    return anchor_links

# Function to check for the proper website link
def is_url_structured(url):
    print("\nchecking for valid url structure")
    has_special_chars = re.search(r'[?&=]', url)
    has_underscore = "_" in url
    has_space = " " in url or "%20" in url
    has_dynamic_params = re.search(r'\?.+=.+', url)
    has_uppercase_chars = re.search(r'[A-Z]', url)

    # Check if any of these characters are present
    if has_special_chars or has_underscore or has_space or has_dynamic_params or has_uppercase_chars or len(url) > 120:
        return False

    return True

# Function get the count of backlinks for the url
def get_backlinks_count(url):
    print("\nchecking for backlinks count")
    req_url = 'https://api.dataforseo.com/v3/backlinks/backlinks/live'
    post_array = [{
        "target": url,
        "mode": "as_is",
        "filters": ["dofollow", "=", True],
        "limit": 1
    }]

    response = requests.post(req_url, json=post_array, auth=(dfs_login, dfs_password))
    response = response.json()

    if len(response["tasks"]):
        task = response["tasks"][0]
        if(len(task["result"])):
            result = task["result"][0]
            return result["total_count"]

    return 0

# Function to get the website rank in google search engine given a keyword
def extract_website_rank_with_keyword(query="", location="India", target_website="", start=0, max_pages=5):
    """
    Function to extract website rankings and search for a specific website.
    It will continue to search through the pages until the target website is found or until the max pages limit is reached.

    Parameters:
    - serp_client: API key for SerpApi
    - query: Search query (default is "best doctor in jaipur")
    - location: Search location (default is "Jaipur, Rajasthan, India")
    - target_website: The specific website URL to search for (default is None)
    - start: The starting index for the search results (default is 0)
    - max_pages: The maximum number of pages to search through (default is 5)

    Returns:
    - Dictionary containing the search result or None if the website is not found
    """
    print(f"extracting rank for keyword '{query}' in {target_website}")
    target_website = target_website.lower() if target_website else None
    current_start = start  # Start index (0 for first page, 10 for second page, etc.)

    while current_start < ((start + max_pages) * 10):
        params = {
            "api_key": serp_api_key,            # API key to authenticate requests
            "engine": "google",                 # Search engine, always Google
            "q": query,                         # Search query
            "location": location,               # Location to customize the search
            "google_domain": "google.com",      # Google domain to target
            "gl": "us",                         # Geolocation setting
            "hl": "en",                         # Language for the search results
            "start": current_start              # Start index for pagination
        }

        # Create the GoogleSearch object and retrieve the data
        search = GoogleSearch(params)
        results = search.get_dict()

        # Check if there are results
        if "organic_results" in results:
            # Loop through the results on the current page
            for rank, result in enumerate(results["organic_results"], start=current_start + 1):
                website_url = result.get("link", "").lower()
                if target_website and target_website in website_url:
                    print(f"Found '{target_website}' at rank {rank}")
                    return rank  # Return the entire result (or customize as needed)

        # Move to the next page by incrementing the start index by 10
        current_start += 10

    print(f"'{target_website}' not found within {max_pages} pages starting from {start}.")
    return None  # Return None if the website is not found

# Function to interact with the LLM for getting user keyword
def get_keyword_using_llm_json(message):
    """Use the LLM to get keyword(s) from the user input"""
    instructions = (
        "Extract only the search phrase from this message."
        "Remove unnecessary words like 'analyze', 'find', 'SEO information', 'with respect to', etc."
        "If there are multiple search phrases, return the one which is like a search phrase rather than a proper noun or a website name"
        "The search phrase is a part of the user message itself. Do not modify the search phrase before returning it. Do not add the location information to the search phrase"
        "Please return the following as a JSON object with the keys 'search_phrase' and 'user_intent':"

        "1- a search phrase"
        "2- user intent of the search"
        "please return only the JSON object. if you don't find any values, return only the key with no values"
    )

    messages = [
        {"role": "system", "content": "You are an SEO expert."},
        {"role": "user", "content": instructions},
        {"role": "user", "content": message}
    ]

    try:
        # Call LLM for Getting URL
        response = GROQ_LLM.invoke(
            input=messages,
            temperature=.0001,
            max_tokens=1024,
            top_p=1,
        )

        response_json= json.loads(response.content)
        return response_json  # Return the response from LLM

    except Exception as e:
        return f"Error: {str(e)}"

# Function to interact with the LLM for getting user mentioned location
def get_location_using_llm_json(message):
    """Use the LLM to get location from the user input"""
    instructions = (
        "Extract only the user location from this message."
        "Do not return anything other than the detected user location from the message in the response"
        "Please format the output as City,State,Country strictly. Give a grammatically correct output without extra commas"
        "Please return the following as a JSON object with the key 'user_location':"

        "1- the user location"
        "please return only the JSON object. if you don't find any values, return only the key with no values"
    )

    messages = [
        {"role": "system", "content": "You are an SEO expert."},
        {"role": "user", "content": instructions},
        {"role": "user", "content": message}
    ]

    try:
        # Call LLM for Getting URL
        response = GROQ_LLM.invoke(
            input=messages,
            temperature=.0001,
            max_tokens=1024,
            top_p=1,
        )

        response_json= json.loads(response.content)
        return response_json # Return the response from LLM

    except Exception as e:
        return f"Error: {str(e)}"

# Function to decide on the website link from user
def get_website_link_to_analyze(input_website_link):
    print("\ngetting website urls from llm")
    urls = get_website_url_using_llm_json(input_website_link).get("urls", [])
    print(urls)
    if not len(urls):
        print("\nPlease provide a valid website link")
        return None
    urls_valid = get_valid_urls(urls)
    print(urls_valid, "url list")
    if urls_valid:
        website_link = finalize_website_link(urls_valid)
        return website_link
    else:
        return "I apologize for the inconvenience. I couldn't scrape the website due to some exceptions."

# Function to get all technical seo related metrics for the website
def get_technical_seo_metrics(url):
    broken_pages = check_broken_links(url)

    metrics = {
        "ssl": is_secured_website(url),
        "indexed": is_indexed_website(url),
        **get_page_speed_metrics(url),
        "mobile_friendliness": get_mobile_friendliness_category(url),
        "xml_exists": is_sitemap_exist(url),
        "has_broken_links": True if broken_pages != None and len(broken_pages) else False,
        "broken_links": broken_pages,
        "url_structured": is_url_structured(url),
        "backlinks_count": get_backlinks_count(url)
    }

    return metrics

# Function to get meta tags for the input website
def get_meta_tags(url):
    try:
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code != 200:
            print((f"Failed to fetch the website: {url}"))
            return "I apologize for the inconvenience. I couldn't scrape the website as some permissions are denied."

        soup = BeautifulSoup(response.content, 'html.parser')

        # Get title
        title = soup.find('title').text if soup.find('title') else ''

        # Get meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        description = meta_desc['content'] if meta_desc else ''

        # Get keywords
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        keywords = meta_keywords['content'] if meta_keywords else ''

        return {
            'Title': title,
            'Description': description,
            'Keywords': keywords
        }

    except requests.RequestException as e:
        return "I apologize for the inconvenience. I couldn't scrape the website due to some exceptions."

# Function to get meta keyword based page rank for the website
def get_meta_keyword_page_rank(url):
    meta_data = get_meta_tags(url)

    keyword_ranks = {}
    metaKeywords = list(filter(lambda key: key != '', meta_data.get('Keywords').split(",")))

    for keyword in metaKeywords:
        rank = extract_website_rank_with_keyword(query=keyword.strip(), target_website=url, max_pages=20)
        keyword_ranks[keyword] = rank

    return keyword_ranks

# Function to perform content analysis of the website using RAG
def content_evaluator(message):
  instructions = (
      "You are an assistant for question-answering tasks related to content quality of a user provided {url} with respect to a user provided {keyword} and the inferred user intent."
      "Use the rating scale lowest, low, medium, high."
      "Keep the answer concise, and always provide a rating."
      "Provide the top 3 strengths and the top 3 areas of improvement along with your answer and rating, which explain the reason for your rating"
      "Structure your answer as follows:"
      "--Overall Rating"
      "--Overall Summary"
      "--Top 3 strengths"
      "--Top 3 areas of improvement"
      "Use the following retrieved context to answer"
      "the question. If you don't know the answer, say that you"
      "don't know."
      "\n\n"
      "{context}"
  )

  messages = [
      {"role": "system", "content": "You are a content quality analyzer."},
      {"role": "user", "content": instructions},
      {"role": "user", "content": message}
  ]

  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", instructions),
          ("human", message),
        ]
  )


  question_answer_chain = create_stuff_documents_chain(GROQ_LLM, prompt)
  rag_chain = create_retrieval_chain(retriever, question_answer_chain)
  response = rag_chain.invoke({"input":message, "url": RunnablePassthrough(), "keyword": RunnablePassthrough()})
  print(response)
  return response["answer"]

# Function to get the website rank by taking url, location and forming keywords using llm and also to return the top 5 sites for those
def extract_website_rank_and_top_sites(message, location="India", target_website="", start=0, max_pages=5):
    """
    Function to extract website rankings and search for a specific website.
    It will continue to search through the pages until the target website is found or until the max pages limit is reached.

    Parameters:
    - serp_client: API key for SerpApi
    - query: Search query (default is "best doctor in jaipur")
    - location: Search location (default is "Jaipur, Rajasthan, India")
    - target_website: The specific website URL to search for (default is None)
    - start: The starting index for the search results (default is 0)
    - max_pages: The maximum number of pages to search through (default is 5)

    Returns:
    - Dictionary containing the search result or None if the website is not found
    """

    location_from_llm = get_location_using_llm_json(message)
    print(f"location from llm: {location_from_llm}")
    location = location_from_llm["user_location"]
    print(f"FINAL location from llm: {location}")
    url_from_llm = get_website_url_using_llm_json(message)
    target_website = url_from_llm["urls"][0]
    print(f"FINAL url from llm: {target_website}")
    search_phrase = get_keyword_using_llm_json(message)
    query = search_phrase["search_phrase"]
    print(f"FINAL Search Phrase query from llm: {query}")

    print(f"extracting rank for keyword '{query}' in {target_website} from location {location}")
    target_website = target_website.lower() if target_website else None
    current_start = start  # Start index (0 for first page, 10 for second page, etc.)
    final_rank=0
    top_5_websites=[]


    while ((current_start < (start + max_pages * 10)) and (final_rank==0)):
        params = {
            "api_key": serp_api_key,            # API key to authenticate requests
            "engine": "google",                 # Search engine, always Google
            "q": query,                         # Search query
            "location_requested": location,     # Location to customize the search
            "google_domain": "google.com",      # Google domain to target
            "gl": "us",                         # Geolocation setting
            "hl": "en",
            "device":"desktop",                 # Language for the search results
            "start": current_start              # Start index for pagination
        }
        print(f"params: {params}")

        # Create the GoogleSearch object and retrieve the data
        search = GoogleSearch(params)
        results = search.get_dict()

        if "organic_results" in results:
          for rank, result in enumerate(results["organic_results"], start=current_start + 1):
              print(result.get("link"))
              website_url = result.get("link", "").lower()
              if rank <= 5:
                top_5_wbebsites = top_5_websites.append(website_url)

              if target_website and target_website == website_url:
                  print(f"Found '{target_website}' at rank {rank}")
                  final_rank=rank
                  print(f'top 5 \n{top_5_websites}')

        # Move to the next page by incrementing the start index by 10
        current_start += 10

    return top_5_websites, final_rank  # Return None if the website is not found

# Function to compare urls across various metrics and use llm to summarize it
def get_analysis_for_urls(message):
    urls_extract = get_website_url_using_llm_json(message)
    print(urls_extract)

    results = {}

    for url in urls_extract.get("urls", []):
        seo_metrics = get_technical_seo_metrics(url)
        meta_data = get_meta_tags(url)
        content_summary = content_evaluator(url)
        rank = extract_website_rank_and_top_sites(url)[1]

        meta_data = meta_data if type(meta_data) is dict else {}

        results[url] = {
            **seo_metrics,
            **meta_data,
            "content": content_summary,
            "rank": rank
        }

    print(results)
    prompt = f"Compare and Summarize the below json on all the parameters provided as best as you can: {results}. NOTE: For the content provide the content overall rating for both websites as well as the content overall summary with strengths and weaknesses"

    response = GROQ_LLM.invoke(
            input=prompt,
            temperature=.0001,
            top_p=1,
    )

    return response.content

print("loaded helper function")

# Loading PDF File for content evaluation guidelines
file_path = "https://static.googleusercontent.com/media/guidelines.raterhub.com/en//searchqualityevaluatorguidelines.pdf"
loader = PyPDFLoader(file_path)

docs = loader.load()

print(len(docs))

# Embedding the read document and vectorizing it for feeding it to RAG
embed_model = FastEmbedEmbeddings(model_name="BAAI/bge-base-en-v1.5")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = InMemoryVectorStore.from_documents(
    documents=splits, embedding=embed_model
)

retriever = vectorstore.as_retriever()

print("retrieved doc")

# Wrap each tool in the Tool class
tools = [
    Tool(name="Check URL Validity", func=is_url_not_found, description="Checks whether given URL lands on 404 page or throws any exception and returns true considering the URL is not valid, otherwise returns false meaning the URL is valid"),
    Tool(name="Get Website URL", func=get_website_link_to_analyze, description="Uses the LLM to get the possible website links from the web for the given input keyword and checks for the valid links which are not landing in 404 page and returns the list of urls to the user to choose"),
    Tool(name="Evaluate Website Technical SEO", func=get_technical_seo_metrics, description="Measures the security, indexing, page speed metric, mobile friendliness of the website, broken link existence, url structure, xml existence, backlinks count for the given input webpage"),
    Tool(name="Get Website Meta Info", func=get_meta_tags, description="Fetches the meta data information of the webpage likely meta title, meta keywords, meta description"),
    Tool(name="Evaluate Website Content Quality", func=content_evaluator, description="Evaluates the quality of the content of the page based on the Retrieval Augmented Generation(RAG)"),
    Tool(name="Get Website Rank and top ranked websites", func=extract_website_rank_and_top_sites, description="Checks for the rank of the website and returns the top 5 websites for the input query based on Google Search results"),
    Tool(name="Compare 2 websites on various SEO Parameters", func=get_analysis_for_urls, description="Compares seo metrics, meta data, content and rank for the two websites provided by the user"),
    Tool(name="Get page rank of meta keywords", func=get_meta_keyword_page_rank, description="Ranks the website based on the list of meta keywords using Google Search results")
]

# Prompt for the LLM
character_prompt = '''
You are an SEO Expert.
Answer the following questions as best you can. Stick to the instructions provided.

You have access to the following tools:

{tools}

IMPORTANT INSTRUCTIONS:
- For Questions that are out of tools scope or any Greetings, don't use the tools. Use your knowledge to respond appropriately. Just stop the execution.
- Strictly adhere to the input given at the moment.
- When you have the website URL, please check whether it is valid or not.
- If the website URL is incomplete or invalid, use the appropriate tool to retrieve the correct website URL.
- If any tool returns None, immediately stop execution and respond with an appropriate message specific to that tool.
- If any tool returns a message, respond with the message returned. Don't add extra messages. Don't proceed. Just stop the execution.
- If Get Website URL tool returns list of length more than 1 and you are unsure of which url to use, return all the urls as a bullet points in response with proper message. Don't choose yourself. Don't proceed. Just stop the execution.
- Once the website URL is valid, use the appropriate tool as per the input or from recent history. 
- You may have to use one or more tools depending on the user input.
- Use the tool description to decide which tool or tools to use.
- If the user asks you what you can do, provide a toolwise description from each of the tools.
- Return the complete response for all the tools that you are using, and not just of the last tool. Don't skip over any tool's response. Make sure that your response is meaningful. Convert any json information into full meaningful sentences.
- Do not make any modifications to the responses for the Evaluate Website Content Quality tool and Compare 2 websites on various SEO Parameters tool. Return the complete response for these tools without summarizing.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}'''

# Initializing the Agent with Tools and Memory
prompt = PromptTemplate.from_template(character_prompt)
agent = create_react_agent(GROQ_LLM, tools, prompt)

memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key="output")
agent_chain = AgentExecutor(agent=agent,
                            tools=tools,
                            memory=memory,
                            max_iterations=10,
                            handle_parsing_errors=True,
                            verbose=True,
                            ).with_config({"run_name": "Agent"})

# Gradio Implementation
def add_message(prompt, messages):
    messages.append(gr.ChatMessage(role="user", content=prompt))
    return messages

async def interact_with_langchain_agent(prompt, messages):
    async for chunk in agent_chain.astream(
        {"input": prompt}
    ):
        if "steps" in chunk:
            for step in chunk["steps"]:
                messages.append(gr.ChatMessage(role="assistant", content=step.action.log,
                                  metadata={"title": f"ðŸ› ï¸ Used tool {step.action.tool}"}))
                yield messages
        if "output" in chunk:
            messages.append(gr.ChatMessage(role="assistant", content=chunk["output"]))
            yield messages

with gr.Blocks(fill_height=True) as demo:
    gr.Markdown("# Chat with a SEO Agent")
    chatbot = gr.Chatbot(
        type="messages",
        label="Agent",
        scale=1,
        avatar_images=(
            None,
            "./bot-3.png",
        ),
    )
    input = gr.Textbox(placeholder="Chat Message", interactive=True, show_label=False, submit_btn=True, autofocus=True)

    # Appending user message to chat history and making the input box disabled
    user_msg = input.submit(add_message, [input, chatbot], [chatbot]).then(lambda: gr.Textbox(interactive=False), None, [input])

    # Streaming and appending the Bot message to chat history
    bot_msg = user_msg.then(interact_with_langchain_agent, [input, chatbot], chatbot)

    # Enabling back the input box and and resetting the value
    bot_msg.then(lambda: gr.Textbox(value="", interactive=True, autofocus=True), None, [input])

demo.launch(debug=True, share=True)